{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCs3JtJ8HN02rKtbW0gLrX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FennecLadd/multilingual_content_generator/blob/main/multilingual_content_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sxS74_Db0sw",
        "outputId": "36eba6e3-693f-4708-bd9c-7f39cfcf62bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, pyngrok, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, fastapi\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed fastapi-0.115.12 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyngrok-7.2.3 starlette-0.46.1 uvicorn-0.34.0\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install transformers nltk spacy fastapi uvicorn pyngrok tensorflow torch\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Download necessary NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Create project directory structure\n",
        "!mkdir -p models api utils data config\n",
        "\n",
        "# Import commonly used libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from transformers import pipeline, MarianMTModel, MarianTokenizer\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile models/content_generator.py\n",
        "from transformers import pipeline, set_seed\n",
        "import torch\n",
        "\n",
        "class ContentGenerator:\n",
        "    def __init__(self, model_name=\"gpt2\"):\n",
        "        \"\"\"Initialize the content generator with a pre-trained model\"\"\"\n",
        "        self.generator = pipeline('text-generation', model=model_name)\n",
        "        set_seed(42)  # For reproducibility\n",
        "\n",
        "    def generate_content(self, prompt, max_length=250, num_return_sequences=1):\n",
        "        \"\"\"Generate content based on the given prompt\"\"\"\n",
        "        try:\n",
        "            generated_texts = self.generator(\n",
        "                prompt,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=num_return_sequences,\n",
        "                pad_token_id=50256\n",
        "            )\n",
        "\n",
        "            return [item['generated_text'] for item in generated_texts]\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating content: {str(e)}\")\n",
        "            return []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psoWrKdsexRl",
        "outputId": "69b601c4-b237-4ed7-b982-07bae1c21af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting models/content_generator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile models/local_translator.py\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class LocalTranslator:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the local translator\"\"\"\n",
        "        self.models = {}\n",
        "        self.tokenizers = {}\n",
        "\n",
        "        # Map of language codes to model names\n",
        "        self.language_map = {\n",
        "            \"es\": \"en-es\",\n",
        "            \"fr\": \"en-fr\",\n",
        "            \"de\": \"en-de\",\n",
        "            \"it\": \"en-it\",\n",
        "            \"ru\": \"en-ru\",\n",
        "            \"zh\": \"en-zh\"\n",
        "        }\n",
        "\n",
        "    def _load_model_for_language(self, target_language):\n",
        "        \"\"\"Load the model for the specified language pair if not already loaded\"\"\"\n",
        "        if target_language not in self.models:\n",
        "            # Map language code to model name\n",
        "            lang_pair = self.language_map.get(target_language, f\"en-{target_language}\")\n",
        "            model_name = f\"Helsinki-NLP/opus-mt-{lang_pair}\"\n",
        "\n",
        "            try:\n",
        "                # Load tokenizer and model\n",
        "                self.tokenizers[target_language] = MarianTokenizer.from_pretrained(model_name)\n",
        "                self.models[target_language] = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "                # Move to GPU if available\n",
        "                if torch.cuda.is_available():\n",
        "                    self.models[target_language].to('cuda')\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading model for {target_language}: {str(e)}\")\n",
        "                # Fallback to a similar language or generic model\n",
        "                if target_language not in [\"es\", \"fr\", \"de\"]:\n",
        "                    print(\"Falling back to Spanish model\")\n",
        "                    return self._load_model_for_language(\"es\")\n",
        "                raise\n",
        "\n",
        "        return self.models[target_language], self.tokenizers[target_language]\n",
        "\n",
        "    def translate_text(self, text, source_language=\"en\", target_language=\"es\"):\n",
        "        \"\"\"\n",
        "        Translate text to the target language\n",
        "\n",
        "        Args:\n",
        "            text: The text to translate\n",
        "            source_language: The source language code (default: en)\n",
        "            target_language: The target language code (e.g., 'es' for Spanish)\n",
        "\n",
        "        Returns:\n",
        "            The translated text\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load the appropriate model\n",
        "            model, tokenizer = self._load_model_for_language(target_language)\n",
        "\n",
        "            # Prepare the text inputs\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "            # Move to GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "                inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
        "\n",
        "            # Generate translation\n",
        "            output = model.generate(**inputs)\n",
        "\n",
        "            # Decode the output\n",
        "            translated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            return translated\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {str(e)}\")\n",
        "            return f\"[Translation Error: {str(e)}]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEn0oTbqe4ev",
        "outputId": "047103c0-ab06-406a-bc24-d2be00172580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting models/local_translator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile models/sentiment.py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "class SentimentAnalyzer:\n",
        "    def __init__(self, use_nltk=True):\n",
        "        \"\"\"\n",
        "        Initialize the sentiment analyzer\n",
        "\n",
        "        Args:\n",
        "            use_nltk: If True, use NLTK's VADER for sentiment analysis instead of TensorFlow\n",
        "        \"\"\"\n",
        "        self.use_nltk = use_nltk\n",
        "\n",
        "        if use_nltk:\n",
        "            self.sia = SentimentIntensityAnalyzer()\n",
        "        else:\n",
        "            self.max_words = 10000\n",
        "            self.max_sequence_length = 200\n",
        "            self.tokenizer = None\n",
        "            self.model = None\n",
        "\n",
        "    def prepare_data(self, texts, labels):\n",
        "        \"\"\"Prepare text data for training a TensorFlow model\"\"\"\n",
        "        if self.use_nltk:\n",
        "            print(\"Using NLTK - no data preparation needed\")\n",
        "            return None, None\n",
        "\n",
        "        # Create and fit tokenizer\n",
        "        self.tokenizer = Tokenizer(num_words=self.max_words)\n",
        "        self.tokenizer.fit_on_texts(texts)\n",
        "\n",
        "        # Convert texts to sequences\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "        # Pad sequences\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=self.max_sequence_length)\n",
        "\n",
        "        return padded_sequences, np.array(labels)\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Build the TensorFlow sentiment analysis model\"\"\"\n",
        "        if self.use_nltk:\n",
        "            print(\"Using NLTK - no model building needed\")\n",
        "            return None\n",
        "\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Embedding(self.max_words, 128, input_length=self.max_sequence_length),\n",
        "            tf.keras.layers.SpatialDropout1D(0.2),\n",
        "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
        "            tf.keras.layers.Dense(32, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.5),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        self.model.compile(loss='binary_crossentropy',\n",
        "                          optimizer='adam',\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=5, batch_size=64, validation_split=0.2):\n",
        "        \"\"\"Train the TensorFlow sentiment analysis model\"\"\"\n",
        "        if self.use_nltk:\n",
        "            print(\"Using NLTK - no training needed\")\n",
        "            return None\n",
        "\n",
        "        if self.model is None:\n",
        "            self.build_model()\n",
        "\n",
        "        history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=validation_split\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict_sentiment(self, text):\n",
        "        \"\"\"\n",
        "        Predict sentiment for the given text\n",
        "\n",
        "        Args:\n",
        "            text: Text to analyze\n",
        "\n",
        "        Returns:\n",
        "            Sentiment score (0-1 where higher is more positive)\n",
        "        \"\"\"\n",
        "        if self.use_nltk:\n",
        "            # Use NLTK's VADER\n",
        "            scores = self.sia.polarity_scores(text)\n",
        "            # Convert VADER's compound score (-1 to 1) to 0-1 scale\n",
        "            return (scores['compound'] + 1) / 2\n",
        "\n",
        "        else:\n",
        "            # Use TensorFlow model\n",
        "            if self.model is None or self.tokenizer is None:\n",
        "                raise ValueError(\"Model not trained. Call train first or use NLTK.\")\n",
        "\n",
        "            # Preprocess the text\n",
        "            sequence = self.tokenizer.texts_to_sequences([text])\n",
        "            padded = pad_sequences(sequence, maxlen=self.max_sequence_length)\n",
        "\n",
        "            # Make prediction\n",
        "            prediction = self.model.predict(padded)[0][0]\n",
        "\n",
        "            return float(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWO1g7u-e9_W",
        "outputId": "fc5597be-280c-441f-cfd2-1ae7e7a1815d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting models/sentiment.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils/nlp_utils.py\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "class EntityExtractor:\n",
        "    def __init__(self, model=\"en_core_web_sm\"):\n",
        "        \"\"\"Initialize spaCy for entity extraction\"\"\"\n",
        "        self.nlp = spacy.load(model)\n",
        "\n",
        "    def extract_entities(self, text):\n",
        "        \"\"\"\n",
        "        Extract named entities from text\n",
        "\n",
        "        Args:\n",
        "            text: Text to analyze\n",
        "\n",
        "        Returns:\n",
        "            List of entities with type and other metadata\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        entities = []\n",
        "        for ent in doc.ents:\n",
        "            entity = {\n",
        "                \"name\": ent.text,\n",
        "                \"type\": ent.label_,\n",
        "                \"start_char\": ent.start_char,\n",
        "                \"end_char\": ent.end_char,\n",
        "                \"description\": spacy.explain(ent.label_)\n",
        "            }\n",
        "            entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_keywords(self, text, top_n=5):\n",
        "        \"\"\"\n",
        "        Extract keywords from text using noun phrases and frequency\n",
        "\n",
        "        Args:\n",
        "            text: Text to analyze\n",
        "            top_n: Number of keywords to return\n",
        "\n",
        "        Returns:\n",
        "            List of top keywords\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Extract noun chunks and named entities\n",
        "        keywords = {}\n",
        "\n",
        "        # Add noun chunks (noun phrases)\n",
        "        for chunk in doc.noun_chunks:\n",
        "            if chunk.text.lower() not in keywords:\n",
        "                keywords[chunk.text.lower()] = 1\n",
        "            else:\n",
        "                keywords[chunk.text.lower()] += 1\n",
        "\n",
        "        # Add named entities\n",
        "        for ent in doc.ents:\n",
        "            if ent.text.lower() not in keywords:\n",
        "                keywords[ent.text.lower()] = 2  # Give entities slightly higher weight\n",
        "            else:\n",
        "                keywords[ent.text.lower()] += 2\n",
        "\n",
        "        # Sort by frequency and return top N\n",
        "        sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
        "        return [item[0] for item in sorted_keywords[:top_n]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRCSRWKCfAve",
        "outputId": "f03226fc-abfc-4845-dd2b-5a90db7a5c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utils/nlp_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Optional\n",
        "import uvicorn\n",
        "\n",
        "# Import our project components\n",
        "from models.content_generator import ContentGenerator\n",
        "from models.local_translator import LocalTranslator\n",
        "from models.sentiment import SentimentAnalyzer\n",
        "from utils.nlp_utils import EntityExtractor\n",
        "\n",
        "# Create FastAPI app\n",
        "app = FastAPI(title=\"Multilingual Content Generator\")\n",
        "\n",
        "# Initialize components\n",
        "content_generator = ContentGenerator()\n",
        "translator = LocalTranslator()\n",
        "sentiment_analyzer = SentimentAnalyzer(use_nltk=True)\n",
        "entity_extractor = EntityExtractor()\n",
        "\n",
        "# Define request/response models\n",
        "class ContentRequest(BaseModel):\n",
        "    prompt: str\n",
        "    max_length: int = 250\n",
        "    target_languages: List[str] = [\"es\", \"fr\", \"de\"]\n",
        "    analyze_sentiment: bool = True\n",
        "    enhance_content: bool = False\n",
        "\n",
        "class TranslationResult(BaseModel):\n",
        "    language: str\n",
        "    translated_text: str\n",
        "    sentiment_score: Optional[float] = None\n",
        "\n",
        "class ContentResponse(BaseModel):\n",
        "    original_content: str\n",
        "    translations: List[TranslationResult]\n",
        "    entities: Optional[List[dict]] = None\n",
        "\n",
        "@app.post(\"/generate\", response_model=ContentResponse)\n",
        "async def generate_content(request: ContentRequest):\n",
        "    \"\"\"Generate content based on prompt and translate to target languages\"\"\"\n",
        "    try:\n",
        "        # Generate content\n",
        "        generated_texts = content_generator.generate_content(\n",
        "            request.prompt,\n",
        "            max_length=request.max_length\n",
        "        )\n",
        "\n",
        "        if not generated_texts:\n",
        "            raise HTTPException(status_code=500, detail=\"Failed to generate content\")\n",
        "\n",
        "        original_content = generated_texts[0]\n",
        "\n",
        "        # Process translations\n",
        "        translations = []\n",
        "        for lang in request.target_languages:\n",
        "            translated_text = translator.translate_text(original_content, target_language=lang)\n",
        "\n",
        "            result = TranslationResult(\n",
        "                language=lang,\n",
        "                translated_text=translated_text\n",
        "            )\n",
        "\n",
        "            # Add sentiment analysis if requested\n",
        "            if request.analyze_sentiment:\n",
        "                sentiment_score = sentiment_analyzer.predict_sentiment(translated_text)\n",
        "                result.sentiment_score = sentiment_score\n",
        "\n",
        "            translations.append(result)\n",
        "\n",
        "        # Process entities if requested\n",
        "        entities = None\n",
        "        if request.enhance_content:\n",
        "            entities = entity_extractor.extract_entities(original_content)\n",
        "\n",
        "        return ContentResponse(\n",
        "            original_content=original_content,\n",
        "            translations=translations,\n",
        "            entities=entities\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=6060, reload=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2Q3XlfffKxC",
        "outputId": "f2f69b62-d659-4112-e46d-aa4c16d1e76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the content generator\n",
        "from models.content_generator import ContentGenerator\n",
        "\n",
        "generator = ContentGenerator()\n",
        "print(\"Testing content generator...\")\n",
        "content = generator.generate_content(\"Write a short paragraph about machine learning\")[0]\n",
        "print(f\"Generated content: {content}\\n\")\n",
        "\n",
        "# Test the translator\n",
        "from models.local_translator import LocalTranslator\n",
        "\n",
        "translator = LocalTranslator()\n",
        "print(\"Testing translator...\")\n",
        "spanish = translator.translate_text(content, target_language=\"es\")\n",
        "print(f\"Spanish translation: {spanish}\\n\")\n",
        "\n",
        "# Test sentiment analysis\n",
        "from models.sentiment import SentimentAnalyzer\n",
        "\n",
        "analyzer = SentimentAnalyzer(use_nltk=True)\n",
        "print(\"Testing sentiment analysis...\")\n",
        "sentiment = analyzer.predict_sentiment(content)\n",
        "print(f\"Sentiment score: {sentiment}\\n\")\n",
        "\n",
        "# Test entity extraction\n",
        "from utils.nlp_utils import EntityExtractor\n",
        "\n",
        "extractor = EntityExtractor()\n",
        "print(\"Testing entity extraction...\")\n",
        "entities = extractor.extract_entities(content)\n",
        "print(f\"Entities: {entities}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zkl5BSgfP1x",
        "outputId": "89388031-b92c-42a7-9ffd-c29dc009dcc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing content generator...\n",
            "Generated content: Write a short paragraph about machine learning:\n",
            "\n",
            "This is an extremely important question—how can we go from a system based on a single, \"one-to-many\" approach to a whole system based on multiple, \"tens of thousands\" approaches? Consider the following examples. Let's assume that there are no such thing as single, two- to three-dimensional structures: this will help with both learning and training. Each of them will be able to generate thousands or even millions of learning objects and can perform thousands of repetitions of training in just a single session.\n",
            "\n",
            "Imagine there were an artificial intelligence platform based on two types of information processing models: one (an \"all-purpose\" training product) and one (one-to-many) training product. This system could be modeled as an input model from another training platform, as well as an input model as an input to a model, but the model would need some sort of ability to learn the specific parts of the input model. Then the machine would try to know the information model for it. This makes it easier to learn to recognize and make different conclusions.\n",
            "\n",
            "Consider the following example. Consider the following information model:\n",
            "\n",
            "A machine learning system is\n",
            "\n",
            "Testing translator...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spanish translation: Consideremos un breve párrafo sobre el aprendizaje automático: Esta es una pregunta extremadamente importante: ¿cómo podemos pasar de un sistema basado en un enfoque único, \"uno a muchos\" a un sistema completo basado en enfoques múltiples, \"decenas de miles\"? Consideremos los siguientes ejemplos. Supongamos que no hay tal cosa como estructuras únicas, de dos a tres dimensiones: esto ayudará con el aprendizaje y la formación. Cada uno de ellos será capaz de generar miles o incluso millones de objetos de aprendizaje y puede realizar miles de repeticiones de formación en una sola sesión. Imaginemos que hubo una plataforma de inteligencia artificial basada en dos tipos de modelos de procesamiento de información: uno (un producto de formación \"todo\" y uno (uno a muchos) producto de formación. Este sistema podría modelarse como un modelo de entrada de otra plataforma de formación, así como un modelo de entrada como una aportación a un modelo, pero el modelo necesitaría algún tipo de capacidad para aprender las partes específicas del modelo de entrada.\n",
            "\n",
            "Testing sentiment analysis...\n",
            "Sentiment score: 0.93135\n",
            "\n",
            "Testing entity extraction...\n",
            "Entities: [{'name': 'one', 'type': 'CARDINAL', 'start_char': 137, 'end_char': 140, 'description': 'Numerals that do not fall under another type'}, {'name': 'tens of thousands', 'type': 'CARDINAL', 'start_char': 197, 'end_char': 214, 'description': 'Numerals that do not fall under another type'}, {'name': 'two- to three', 'type': 'CARDINAL', 'start_char': 314, 'end_char': 327, 'description': 'Numerals that do not fall under another type'}, {'name': 'thousands', 'type': 'CARDINAL', 'start_char': 438, 'end_char': 447, 'description': 'Numerals that do not fall under another type'}, {'name': 'millions', 'type': 'CARDINAL', 'start_char': 456, 'end_char': 464, 'description': 'Numerals that do not fall under another type'}, {'name': 'thousands', 'type': 'CARDINAL', 'start_char': 501, 'end_char': 510, 'description': 'Numerals that do not fall under another type'}, {'name': 'two', 'type': 'CARDINAL', 'start_char': 629, 'end_char': 632, 'description': 'Numerals that do not fall under another type'}, {'name': 'one', 'type': 'CARDINAL', 'start_char': 673, 'end_char': 676, 'description': 'Numerals that do not fall under another type'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2vLmosBsMmZiJMYnQMfb36X5LDw_5t2EqCfLt58aqX2YGVtNT\")\n"
      ],
      "metadata": {
        "id": "QRhDn1WLiJCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# Apply nest_asyncio to make asyncio work in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Start the FastAPI app in a separate thread\n",
        "def run_app():\n",
        "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=6060)\n",
        "\n",
        "# Start the thread\n",
        "thread = threading.Thread(target=run_app, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "# Give the server a moment to start\n",
        "time.sleep(2)\n",
        "\n",
        "# Create a tunnel\n",
        "public_url = ngrok.connect(6060)\n",
        "print(f\"FastAPI app is running at: {public_url}\")\n",
        "\n",
        "print(f\"You can access the API documentation at {public_url}/docs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkM2tFlzfnjE",
        "outputId": "e876a975-65bc-4d06-e33a-98fca566f184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [13095]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:6060 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI app is running at: NgrokTunnel: \"https://f441-35-229-240-69.ngrok-free.app\" -> \"http://localhost:6060\"\n",
            "You can access the API documentation at NgrokTunnel: \"https://f441-35-229-240-69.ngrok-free.app\" -> \"http://localhost:6060\"/docs\n"
          ]
        }
      ]
    }
  ]
}